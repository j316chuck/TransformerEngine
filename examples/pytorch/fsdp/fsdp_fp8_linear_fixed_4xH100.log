[2023-12-15 16:12:47,971] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2023-12-15 16:12:47,971] torch.distributed.run: [WARNING] 
[2023-12-15 16:12:47,971] torch.distributed.run: [WARNING] *****************************************
[2023-12-15 16:12:47,971] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2023-12-15 16:12:47,971] torch.distributed.run: [WARNING] *****************************************
[GPU-0] WORLD_SIZE = 4
[GPU-0] Linear()
[GPU-0] Iter. 1
[GPU-0] Iter. 2
[GPU-0] Iter. 3

[GPU-0] Training Time: 24.257302734375s
[GPU-0] Avg. Iter. Time: 8.085767578125s
[GPU-0] Max memory allocated = 46205MiB


[GPU-1] Training Time: 24.2569609375s
[GPU-1] Avg. Iter. Time: 8.085653645833334s
[GPU-1] Max memory allocated = 46205MiB


[GPU-2] Training Time: 24.256810546875s
[GPU-2] Avg. Iter. Time: 8.085603515625s
[GPU-2] Max memory allocated = 46205MiB


[GPU-3] Training Time: 24.256482421875s
[GPU-3] Avg. Iter. Time: 8.085494140625s
[GPU-3] Max memory allocated = 46206MiB

