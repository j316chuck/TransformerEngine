[2023-12-15 05:55:46,301] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2023-12-15 05:55:46,301] torch.distributed.run: [WARNING] 
[2023-12-15 05:55:46,301] torch.distributed.run: [WARNING] *****************************************
[2023-12-15 05:55:46,301] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2023-12-15 05:55:46,301] torch.distributed.run: [WARNING] *****************************************
[GPU-0] WORLD_SIZE = 8
[GPU-0] Linear()
[GPU-0] Iter. 1
[GPU-0] Iter. 2
[GPU-0] Iter. 3

[GPU-6] Training Time: 20.61418359375s
[GPU-6] Avg. Iter. Time: 6.87139453125s
[GPU-6] Max memory allocated = 35975MiB


[GPU-4] Training Time: 20.61318359375s
[GPU-4] Avg. Iter. Time: 6.871061197916667s
[GPU-4] Max memory allocated = 35975MiB


[GPU-5] Training Time: 20.614412109375s
[GPU-5] Avg. Iter. Time: 6.8714707031249995s
[GPU-5] Max memory allocated = 35975MiB


[GPU-0] Training Time: 20.6150234375s
[GPU-0] Avg. Iter. Time: 6.871674479166667s
[GPU-0] Max memory allocated = 35975MiB


[GPU-2] Training Time: 20.61443359375s
[GPU-2] Avg. Iter. Time: 6.871477864583333s
[GPU-2] Max memory allocated = 35975MiB


[GPU-7] Training Time: 20.614390625s
[GPU-7] Avg. Iter. Time: 6.8714635416666665s
[GPU-7] Max memory allocated = 35975MiB


[GPU-1] Training Time: 20.6149921875s
[GPU-1] Avg. Iter. Time: 6.8716640625s
[GPU-1] Max memory allocated = 35975MiB


[GPU-3] Training Time: 20.61437109375s
[GPU-3] Avg. Iter. Time: 6.87145703125s
[GPU-3] Max memory allocated = 35975MiB

