[2023-12-15 21:36:47,379] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2023-12-15 21:36:47,379] torch.distributed.run: [WARNING] 
[2023-12-15 21:36:47,379] torch.distributed.run: [WARNING] *****************************************
[2023-12-15 21:36:47,379] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2023-12-15 21:36:47,379] torch.distributed.run: [WARNING] *****************************************
[GPU-0] WORLD_SIZE = 4

[GPU-1] Pre-FSDP memory use = 8590.196736MiB
[GPU-2] Pre-FSDP memory use = 8590.196736MiB
[GPU-3] Pre-FSDP memory use = 8590.196736MiB
[GPU-0] Pre-FSDP memory use = 8590.196736MiB
[GPU-0] Post-FSDP memory use = 2147.549184MiB
[GPU-1] Post-FSDP memory use = 2147.549184MiB
[GPU-3] Post-FSDP memory use = 2147.549184MiB
[GPU-2] Post-FSDP memory use = 2147.549184MiB
[GPU-0] Iter. 1
[GPU-0] Iter. 2
[GPU-0] Iter. 3

[GPU-0] Training Time: 19.655080078125s
[GPU-0] Avg. Iter. Time: 6.551693359375s
[GPU-0] Peak memory use = 46205MiB


[GPU-2] Training Time: 19.6542734375s
[GPU-2] Avg. Iter. Time: 6.551424479166666s
[GPU-2] Peak memory use = 46205MiB


[GPU-3] Training Time: 19.65421875s
[GPU-3] Avg. Iter. Time: 6.551406249999999s
[GPU-3] Peak memory use = 46206MiB


[GPU-1] Training Time: 19.6545625s
[GPU-1] Avg. Iter. Time: 6.551520833333334s
[GPU-1] Peak memory use = 46205MiB

