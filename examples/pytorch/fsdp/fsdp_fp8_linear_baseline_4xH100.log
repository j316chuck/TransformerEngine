[2023-12-15 16:02:27,671] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2023-12-15 16:02:27,671] torch.distributed.run: [WARNING] 
[2023-12-15 16:02:27,671] torch.distributed.run: [WARNING] *****************************************
[2023-12-15 16:02:27,671] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2023-12-15 16:02:27,671] torch.distributed.run: [WARNING] *****************************************
[GPU-0] WORLD_SIZE = 4
[GPU-0] FP8 FSDP OOM workaround DISABLED
[GPU-0] Linear()
[GPU-0] Iter. 1
[GPU-0] Iter. 2
[GPU-0] Iter. 3

[GPU-0] Training Time: 28.088642578125s
[GPU-0] Avg. Iter. Time: 9.362880859375s
[GPU-0] Max memory allocated = 54795MiB


[GPU-1] Training Time: 28.088357421875s
[GPU-1] Avg. Iter. Time: 9.362785807291667s
[GPU-1] Max memory allocated = 54795MiB


[GPU-2] Training Time: 28.088193359375s
[GPU-2] Avg. Iter. Time: 9.362731119791667s
[GPU-2] Max memory allocated = 54795MiB


[GPU-3] Training Time: 28.08786328125s
[GPU-3] Avg. Iter. Time: 9.36262109375s
[GPU-3] Max memory allocated = 54796MiB

